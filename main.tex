\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\title{Rapport Apprentissage Automatique}
\author{Gabin Marc Mberi-Kongo, Quentin Vigne, Vincent Deschaud}
\author{Thomas Cambon, Florent Jakubowski }
\author{
  Gabin Marc Mberi-Kongo\\
  \texttt{gabin.mberi-kongo@univ-lyon2.fr}
  \and
  Quentin Vigné\\
  \texttt{quentin.vigne@univ-lyon2.fr}
  \and
  Vincent Dechaud\\
  \texttt{vincent.dechaud@univ-lyon2.fr}
  \and
  Thomas Cambon\\
  \texttt{t.cambon@univ-lyon2.fr}
  \and
  Florent Jakubowski\\
  \texttt{florent.jakubowski@univ-lyon2.fr}
}
\date{February 2021}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

Dans le cadre du module Apprentissage supervisé dispensé par Monsieur Ah-pine, nous avons eu le choix d'implémenter un algorithme parmi plusieurs et de tester ses performances par rapport à d'autres méthodes d'apprentissage supervisé. 

\section{Sujet}
Parmi les différents sujets proposés nous avons fait le choix de travailler sur le sujet "Adaboost" qui nécéssite d'étudier de façon approfondie deux algorithmes pour la catégorisation de données. Ces deux algorithmes porteront sur la catégorisation binaire et la catégorisation multiclasse. Afin de traiter ce sujet, une première étape de lecture et d'analyse est nécéssaire, notamment par la lecture des articles fondateurs de Adaboost et la prise en connaissances des notions de boosting.  
\section{Problématique scientifique}
La problématique scientifique de ce projet est de vérifier l'intérêt des méthodes dites d'ensembles pour la classification binaire et multiclasse, plus spécifiquement l'algorithme Adaboost. L'enjeu sera d'observer la différence de performances entre des méthodes d'apprentissage supervisées ne se basant pas sur une prédiction par comité et des méthodes qui au contraire utilisent le vote par comité pour faire leurs prédictions.
Nous nous sommes appuyés sur l'implémentation de l'algorithme Adaboost pour données binaires et de sa variante Adaboost M1 (multiclasse) telle que décrite dans l'article de Freund et Schapire \citep{FreundSchapire1996} en 1996 pour reimplémenter nos algorithmes.

\section{Fondements, principes et état de l'art}
Le fondement de Adaboost part du principe d'algorithmes de "Boosting" introduit par Yoav Freund et Robert Schapire en juillet 1996 \citep{FreundSchapire1996} dans l'article "Experiments with a new boosting algorithm". Le but de ce nouvel algorithme de boosting est de réduire significativement l'erreur générée par les classifieurs faibles qui donnent un score à peine meilleur qu'un classifieur aléatoire. Ces classifieurs sont dénommés "classifieurs faibles" ou bien "Weak Learner" dans l'article original. Ce fût la première méthode fonctionnelle permettant de mettre en oeuvre le principe de boosting.

Souvent, le bagging et le boosting sont confondus puisque ces deux méthodes sont des méthodes ensemblistes. Le boosting que nous étudions ici va attribuer un poids plus fort aux classifieurs ayant les meilleures prédictions. Adaboost s'appuie sur ce principe en donnant plus d'importance aux valeurs compliquées à prédire.


\section{Implémentation}
Le but de cette partie est d'expliciter d'une part les détails techniques et l'organisation du code (5.1, 5.2, 5.4) et d'autre part notre organisation en tant qu'équipe et la manière dont nous avons réalisé ce code (5.3).

\subsection{Langage et librairies utilisées}
Nous avons utilisé le langage python pour développer notre code. Ce choix a été motivé par la simplicité d'implémentation et de compréhension du code. De plus, l'accès à de nombreuses librairies comme Pandas ou Scikit-Learn était très intéressant dans notre cas. 

Ci-suit la liste des librairies python utilisées durant ce projet : 
\begin{itemize}
  \item Numpy : Numpy est une bibliothèque que nous utilisons pour le calcul matricielle et sur des tableaux à plusieurs dimensions.
  \item Pandas : Pandas est une bibliothèque permettant de manipuler des données sous Python. Nous nous en servons pour charger des données depuis un csv dans des dataframes pandas et pour effectuer des opérations dessus.
  \item Scikit-learn : Scikit-learn est une bibliothèque destinée à l'apprentissage automatique. Elle fournit de nombreuses classes d'algorithmes d'apprentissage supervisé très simple à manipuler, des fonctions pour séparer son jeu de données en deux jeux de données (entraînement et test dans notre cas) et d'autres fonctions intéressantes permettant de visualiser les résultats de nos entraînements.
  \item Os : Os est un module de base de python, il permet d'utiliser les fonctionnalités dépendantes du système d'exploitation. Nous nous en servons pour définir un répertoire courant au moment de l'éxécution de notre code et ainsi définir un chemin relatif pour charger notre fichier csv.
\end{itemize}

\subsection{Workflow}

Pour le développement du code nous avons adopté une organisation basique semblable aux organisations agiles que l'on peut retrouver dans le milieu professionnel. 
Nous avons divisé le travail en tâches et nous nous les sommes attribuées au fur et à mesure du projet. Nous avions un tableau kanban rudimentaire avec des états 'Nouvelle tâche', 'A faire', 'En cours' et 'Terminé'. Nous passions les items à travers les différentes colonnes en fonction de l'état des tâches. Cela nous a permis de bien nous répartir le travail et de suivre l'avancement du projet. 

Nous étions tous d'accord pour versionner dès le début notre code. Pour ce faire nous avons eu recours a l'outil de versionning Git. 
Pour partager notre code nous nous sommes tournés vers le site Github qui permet un stockage des différentes versions du code dans un dépôt virtuel (repository en anglais), c'est-à-dire hébergé sur l'un de leurs serveurs.

Pour chaque tâche nous réalisions une nouvelle branche sur notre environnement locale puis une fois terminé nous l'envoyions sur le dépôt distant via une demande d'ajout ('Pull request'). 
Le code était ainsi relu par un ou plusieurs camarade(s) pour s'assurer qu'il soit fonctionnel et compréhensible de tous. Une fois cette étape de validation passée nous ajoutions le code au code existant. 

Toute cette organisation nous a permit d'itérer sur le projet et de mettre en commun le code développé par chacun de manière propre et organisée. 

\subsection{Organisation du code}

Dans le dossier src/adaboost réside les différentes implémentations de l'algorithme adaboost avec les fichiers utiles aux chargements des données et à la comparaison avec d'autres algorithmes d'apprentissage supervisé.

Pour l'implémentation nous avons choisis le paradigme de l'orienté objet, c'est-à-dire une approche par classe. Il existe une classe par fichier .py avec pour chacune d'entre elles des méthodes spécifiques. 

Les fichiers main.py et cleanCSV.py sont les seuls qui sont composés de fonctions et non de classes. Le paradigme de la programmation fonctionnelle nous parassait plus adapté étant donné que nous éxécutions, chargions ou tranformions principalement des données grâce à ces fichiers.  

\paragraph{Partie classification binaire : }
\begin{itemize}
  \item weakleaner.py : classe weakleaner
  \item adaboost\_binary.py : classe adaboost\_binary
  \item adaboost\_binary\_benchmark.py : classe benchmark\_performance
  \item cleanCSV\_adaboost\_binary.py : fonction format\_kickstarter\_datas
  \item main\_binary.py : charge le jeu de données et effectue un benchmark de plusieurs algorithmes dessus.
\end{itemize}

\paragraph{Partie classification multiclasse : }


\begin{itemize}
  \item weakleaner\_M1\_multiclass.py : classe weakleaner
  \item adaboostM1\_multiclass.py : classe adaboost\_m1\_multiclass
  \item adaboostM1\_multiclass\_benchmark.py : classe benchmark\_performance
  \item cleanCSV\_M1\_multiclass.py : fonction format\_kickstarter\_datas
  \item main\_multiclass.py : permet de charger les différents datasets et d'effectuer un benchmark de différents algorithmes dessus. 
\end{itemize}


\section{Présentation du jeu de données}

Nous avons choisi d'utiliser un jeu de données avec différentes informations sur des projets Kickstarter de 2018. Kickstarter est une entreprise de financement participatif qui donne la possibilité aux internautes de financer des projets qui n'ont pas encore été développés.\newline 

\noindent Le fichier csv que nous avons utilisé contient les variables suivantes : \newline 
\begin{itemize}
    \item ID du projet (ID)
    \item Nom du projet (NAME)
    \item Catégorie principale (MAIN\_CATEGORY)
    \item Sous-catégorie (CATEGORY)
    \item Monnaie utilisée (CURRENCY)
    \item Date limite pour le financement participatif (DEADLINE)
    \item Somme d'argent (GOAL) que souhaite récolter un créateur pour financer son projet (si la somme n'est pas atteinte, le projet ne sera pas financé et le créateur ne recevra pas d'argent)
    \item Date de lancement du projet (LAUNCHED)
    \item Montant total promis par les contributeurs pour le projet (PLEGED)
    \item Nombre de contributeurs (BACKERS)
    \item Pays (COUNTRY)
    \item Conversion en USD (USD\_PLEGED) du montant total promis par les contributeurs pour le projet (conversion effectuée par kickstarter) 
    \item Conversion en USD (USD\_PLEGED\_REAL) du montant total promis par les contributeurs pour le projet (conversion depuis l'API Fixer.io)
    \item Et enfin somme d'argent que souhaite récolter un créateur pour financer son projet, convertie en USD (USD\_GOAL\_REAL).\newline 

\end{itemize}

\noindent La variable que l'on essayera de prédire est l'état du projet (STATE). Six états sont possibles :

\begin{itemize}
    \item Projet réussi (successfull)
    \item Projet échoué (failed)
    \item Projet en cours (live)
    \item Projet annulé (canceled)
    \item Projet suspendu (suspended)
    \item Ou projet non défini (undefined).\newline 
\end{itemize}

\noindent Voici un extrait des données :

\begin{center} \includegraphics[scale=0.55]{extrait_donnees_partie1.PNG} \end{center}
\begin{center} \includegraphics[scale=0.55]{extrait_donnees_partie2.PNG} \end{center}
\noindent Ce jeu de données est intéressant car il a un nombre important d'individus et de variables. De plus, il possède à la fois des variables catégorielles et numériques, ce qui donne de la variété dans les types de variables. \newline

\noindent Pour la suite, nous avons décidé de supprimer les colonnes ID et NAME car ce sont deux variables qui ont des valeurs uniques pour chacun des projets. Nous avons également supprimé les variables GOAL, PLEGED et USD\_PLEGED pour ne garder qu'une unique version de ces variables avec la devise USD.\newline

\noindent Nous conserverons uniquement les projets à l'état "Successful" ou "Failed" pour l'algorithme adaboost binaire. "Successful" prendra la valeur 1 et "Failed" la valeur 0.\newline

\noindent Pour l'extension adaboost.M1, nous garderons cette fois-ci tous les individus.


\section{Protocole expérimental}

Nous souhaitons comparer nos algorithmes implémentés avec d'autres algorithmes de classification supervisée. Ces algorithmes sont issus de la librairie "Scikit-Learn". 

Les algorithmes choisis vont du plus basique en partant de la régression linéaire et allant vers des algorithmes concurents d'Adaboost tels que Random Forest et Bagging.

Les algorithmes testés sont les suivants :
\begin{itemize}
    \item Régression Linéaire 
    \item Régression Logistique
    \item Arbre de décision
    \item SVM
    \item Random Forest
    \item Bagging
    \item Ada Boost
\end{itemize}

Tous les algorithmes ont été testés sur les mêmes données d'apprentissage et de test et avec le même nombre d'itérations (allant de 1 à 500). 

%Pour le protocole expérimental nous avons choisis plusieurs autres algorithmes d'apprentissage supervisé n'utilisant pas le vote par comité. 

%Pour la classification binaire nous avons choisis les algorithmes suivant : 
%Pour la classification multiclasse nous avons choisis les algorithmes suivant : 

%Nous avons entrainé nos 2 algorithmes et les autres algorithmes avec le même nombre d'époques et le même data set. 
%Puis nous avons comparé leurs prédictions avec les prédictions des autres algorithmes choisis à l'aide d'une fonction d'erreur.

%La fonction d'erreur calcule le pourcentage de mauvaises prédictions données sur les données de test. 

\section{Résultats expérimentaux}
Voici les résultats expérimentaux obtenus par différentes méthodes sur notre jeu de données binaires  :

\begin{table}[htbp]
    \centering
    \small
    \setlength\tabcolsep{2pt}
    \begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    Algorithme & 1 itérations & 10 itérations & 50 itérations & 100 itérations & 200 itérations & 500 itérations \\ \hline
    Régression Linéaire & 0.36087 &  0.36087 &  0.36087 & 0.36087 & 0.36087 & 0.36087 \\ \hline
    Régression Logistique & 0.39232 & 0.05242 & 0.00575 & 0.00595 &  0.00595 & 0.00595 \\ \hline
    SVM & 0.27555 & 0.29000 & 0.14666 & 0.07505 & 0.07636 & 0.12707 \\ \hline
    Arbre de décision & 0.00707 & 0.00676 & 0.00656 & 0.00737 & 0.00686 & 0.00707\\ \hline
    Random Forest &  0.04424 & 0.01555 & 0.01171 & 0.01181 & 0.01060 &  0.01131  \\ \hline
    Bagging & 0.00717 & 0.00494 & 0.00414 & 0.00414 & 0.00424 & 0.00424\\ \hline
    Adaboost Scikit-Learn & 0.16202 & 0.06858 &  0.02050 & 0.01383 & 0.00939 & 0.00575\\ \hline
    Adaboost binaire & 0.16202 & 0.08666 & 0.03161 & 0.02131 & 0.01444 & 0.00919\\
    \hline
    \end{tabular}
    \caption{Taux d'erreur en prédictions sur nos données binaires}
\end{table}

Les trois méthodes qui donnent les meilleurs résulats sont respectivement la régression logistique, l'algorithme Adaboost de Scikit-Learn et les arbres de décision. Notre algorithme Adaboost binaire obtient également de très bons résulats avec un taux d'erreur de 0,00919 après 500 itérations. 

Passons maintenant à l'analyse des résulats de la partie multiclasse :

\begin{table}[htbp]
    \centering
    \small
    \setlength\tabcolsep{2pt}
    \begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    Algorithme & 1 itérations & 100 itérations & 200 itérations \\ \hline
    Régression Logistique & 0.47545 & 0.12959 & 0.12868 \\\hline
    SVM & 0.72979 & 0.58020 & 0.49262 \\ \hline
    Arbres de décision &  0.17949 & 0.17797 & 0.18020  \\ \hline
    Random Forest &  0.21121 & 0.12060 & 0.12161  \\ \hline
    Bagging & 0.19080 & 0.12484 & 0.12363 \\ \hline
    Adaboost Scikit-Learn & 0.26606 & 0.26606 & 0.26606 \\ \hline
    Adaboost M1 & 0.26606 & 0.25868 & 0.25868\\\hline
    \end{tabular}
    \caption{Taux d'erreur en prédictions sur nos données multiclasses}
\end{table}


Au vu des résultats, on peut se demander pourquoi l'erreur ne diminue pas au fil des itérations pour notre implémentation de l'algorithme M1. En effet, celui-ci semble converger et ce seulement après quelques itérations. 
Nous avons constaté qu'après quelques itérations, l'algorithme M1 n'est plus en mesure de produire des classifieurs permettant une estimation supérieur à 0,5. Les classifieurs produits sont ni inférieurs à un tirage aléatoire, ni supérieurs, mais bien égaux durant toute la suite de l'entrainement. Cela entraine une valeur de beta égale à 1 et cela signifie que ces estimateurs produits ne seront pas interrogés au moment du vote en comité. Voilà pourquoi nous nous retrouvons avec un algorithme qui converge après une ou deux itérations. Au delà de ces itérations l'algorithme ne produit plus d'estimateurs plus performant que l'aléatoire.\\ 

Nous nous sommes interrogés sur ce phénomène et nous avons essayé de mieux le comprendre. Pour cela nous avons créer des jeux de données synthétiques grâce à la fonction make\_gaussian\_quantile de scikit-learn. Cette fonction permet de générer des échantillons gaussiens isotropes et d'étiqueter les échantillons par quantile. Le premier contenait 10 dimensions et 3 classes, et le second 3 dimensions et 3 classes. Nous avons eu le même problème que précédemment avec ces données. \\

Nous avons également créer plusieurs datasets de différentes dimensions et contenant 3 classes ou plus grâce à la fonction make\_classification de Scikit-Learn. Cette fonction génère un problème de classification aléatoire de n-classes. Le code peut être trouvé dans le fichier main\_multiclass.py. Il suffit d'utiliser la fonction choose\_dataset avec comme argument la fonction générant le dataset souhaité. Les résultats sont meilleurs qu'avec les datasets précédents basés sur des échantillons gaussiens et sont proches de l'algorithme adaboost de scikit-learn voir même meilleurs parfois. Cependant les résultats restent aléatoires et nous n'avons pas deux fois les mêmes résultats lorsque nous lançons l'entrainement. Cela s'explique par le fait que make\_classification introduit une part d'aléatoire dans sa génération de données. \\

Après plusieurs tests de datasets créés avec make\_classification nous pensons que l'algorithme M1 a du mal à traiter plus de 3 classes et encore plus si ces dernières ne sont pas équilibrées (nombres d'individus par classes très différents les uns des autres).\\ 

Cette observation est en accord avec nos données qui ont des modalités déséquilibrées (52\% de 'failed', 35\% de 'successful', 10\% de 'canceled' et seulement 0.94 \% de 'undefined', 0.73\% de 'live' et 0.48\% de 'suspended'). \\

Nous avons entrepris de programmer l'algorithme Adaboost M2  pour voir s'il venait à bout des effets de bords  et des mauvaises performances constatées avec l'algorithme M1 mais nous n'avons malheureusement pas réussi à aller jusqu'au bout de son implémentation. 

\section{Une discussion scientifique/analyse critique}

Les algorithmes de boosting sont faciles à lire, à implémenter et à interpréter, ce qui rend les interprétations des prédictions facile à manipuler \cite{FreundSchapire1996}. Mais d'un autre coté les algorithmes de boosting soufrent globalement des problèmes d'optimisations et sont fortement influancés par des outliers.\\

Adaboost présente peu d'inconvénients mais il arrive qu'il soit particulièrement vulnérable au bruit uniforme et aux valeurs aberrantes des données \cite{educba}. 
Un autre inconvénient est que la méthode est presque impossible à mettre à l'échelle. En effet, chaque estimateur fonde sa justesse sur les prédicteurs précédents, ce qui rend la procédure difficile à uniformiser \cite{corporatefinanceinstitute}.\\

Il faut aussi noté les effets liés aux problèmes de déséquilibre de classes \cite{Longadage-2013}. En effet, certains auteurs ont constatés que l'algorithme arrive a faire des séparations même pour des classe mineurs sur des données n'ayant presque pas de frontière mais cependant la méthode ne peut pas détecter ce qu'ils ont appeller le "concept à la dérive".\\

Dans notre cas, l'algorithme Adaboost implémenté donne des résultats bien supérieurs aux autres algorithmes que sont la Régression lineaire et les SVM. Sur les 10 premières itérations, un taux d'erreur de 8\% contre respectivement 29\% et 36\% pour les SVM et la Régression Linéaire et à 50 itérations. Ce taux d'erreur baisse de moitié soit à 3\% contre 14\% et 36\% pour les SVM et la Régression Linéaire.\\
Toutefois sachant que Adaboost combine les propriétés d'optimisations notamment avec les bornes sur les erreurs, les propriétés des SVM et les propriétés d'algorithmes bayésiens, les résultats donnés par notre algorithme devrait être au coude à coude voir supérieurs à ceux des arbres de décisons. Nous supposons donc que les résultats sur l'efficacité d'Adaboost dépendent de la qualité des données mais aussi du type de données. N'ayant pas eu le temps nécessaire pour contrôler les valeurs aberrantes, nous ne pouvant pas affirmer avec certitude que nos données sont saines. Une vérification approfondie pourrait peut-être améliorer nos résultats, notamment pour Adaboost M1.

\section{Conclusion}

Ce projet nous aura permis de voir en détail un des algorithmes étudiés en cours. Cela nous a poussé à comprendre plus en détail ce qu'il y derrière les méthodes de boosting d'une part, mais également derrière d'autres méthodes telles que les SVM ou le Bagging.\\

Parmi les principales difficultés rencontrées au cours de ce projet, il y a d'abord eu la recherche du jeu de données : malgré les nombreuses ressources disponibles sur internet, il reste difficile de trouver un jeu de données qui s'adapte parfaitement à ce que nous recherchions.\\

L'implémentation de Adaboost M1 a quant à elle été plus difficile et plus longue que celle de Adaboost binaire car, comme expliqué précédemment, nous avons cherché à comprendre pourquoi notre score ne s'améliorait pas au fil des itérations.\\

Au niveau des pistes d'amélioration de notre travail, la plus intéressante serait sans doute de finir l'implémentation de Adaboost M2 afin de comparer ses résulats avec ceux de Adaboost M1.

\bibliographystyle{plain}
\bibliography{references}
\end{document}
